{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3108089e",
   "metadata": {},
   "source": [
    "### E Karvelis | 5/24/2023\n",
    "### Purpose\n",
    "Test the performance of trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5776a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from transformer_1 import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/karvelis03/dl_kcat/scripts/')\n",
    "from prep_data import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9182765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHelper functions\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110f79f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ModelTest(PathDataset):\n",
    "    # Class that organizes a model, dataset, and \n",
    "    # set of held-out variants (within dataset)\n",
    "    # for execution of various model performance\n",
    "    # analyses\n",
    "    \n",
    "    def __init__(self, model_file, data_file, meta_file=None, output_file=None, scaler=None):\n",
    "        \n",
    "        super().__init__(data_file, meta_file, path_set_size=1)\n",
    "        self.scaler = scaler\n",
    "        self.model_file = model_file\n",
    "        self.config_file = glob('/'.join(model_file.split('/')[0:-1]) + '/*conf*txt')[0]\n",
    "        self.train_vars, self.test_vars = ModelTest.get_val_variants(model_file, output_file=output_file)\n",
    "        \n",
    "        # Create train indexes corresponding to variants in the train set\n",
    "        train_idx = np.nonzero(np.in1d(np.array(self.meta.variant),np.unique(self.train_vars)))[0]\n",
    "\n",
    "        # Load the model\n",
    "        #self.model = TransformerModel(self.data.shape[-1], self.data.shape[-2], d_model=128) ### read d_model from config file\n",
    "        self.model = self.make_model()\n",
    "        self.model.load_state_dict(torch.load(self.model_file))\n",
    "\n",
    "        # Fit the scaler to the training data\n",
    "        if self.scaler == None:\n",
    "            self.scaler = NormalScaler()\n",
    "            self.scaler.fit(self.data[train_idx,:,:])\n",
    "            self.data_scaler = DataScaler(self.scaler)\n",
    "        else:\n",
    "            self.data_scaler = DataScaler(self.scaler)\n",
    "            \n",
    "    def make_model(self):\n",
    "        # Creates the PyTorch model object into which pre-trained\n",
    "        # weights are loaded\n",
    "        d_model = 256\n",
    "        n_head = 4\n",
    "        d_tran_ffn = 1024\n",
    "        dropout_tran_encoder = 0.2\n",
    "        n_tran_layers = 2\n",
    "        d_mlp_head = 128\n",
    "        dropout_mlp_head = 0.2\n",
    "        with open(self.config_file, 'r') as f:\n",
    "            settings = f.read()\n",
    "            exec(settings)\n",
    "        model = TransformerModel(input_size=self.data.shape[-1],\n",
    "                                 input_length=self.data.shape[-2],\n",
    "                                d_model = 128,\n",
    "                                n_head = 2,\n",
    "                                d_tran_ffn = 256,\n",
    "                                dropout_tran_encoder = 0.2,\n",
    "                                n_tran_layers = 1,\n",
    "                                d_mlp_head = 64,\n",
    "                                dropout_mlp_head = 0.2).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def score_var(self, var):\n",
    "        \n",
    "        var_idx = np.nonzero(np.in1d(np.array(self.obs.variant), var))[0]\n",
    "        var_dataset = PathTorchDataset(self, elligible_idxs=var_idx, transform=self.data_scaler)\n",
    "\n",
    "        loader = DataLoader(var_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        self.model.eval()\n",
    "        total_obs = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            results = []\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "\n",
    "                n_obs = batch['paths'].size(0)\n",
    "                output = self.model(batch['paths']).cpu().numpy().reshape((-1))\n",
    "                total_obs += n_obs\n",
    "                # self.results[var][path_set_size].append(output)\n",
    "                results.append(output)\n",
    "    \n",
    "        return results\n",
    "        \n",
    "    def plot_test_var_pred(self, path_set_sizes=[10,100,1000], figname=False):\n",
    "        # Plots the distribution of predicted log(kcat) values \n",
    "        # for each held-out (i.e., validation) variant as \n",
    "        # function of the path_set_size, which is the number of\n",
    "        # paths included in each observation of the variant\n",
    "        \n",
    "        results = {}\n",
    "        for var in self.test_vars:\n",
    "            d = {}\n",
    "            for size in path_set_sizes:\n",
    "                d[size] = []\n",
    "            results[var] = d\n",
    "            \n",
    "        for path_set_size in path_set_sizes:\n",
    "            \n",
    "            print (f'Testing path_set_size: {path_set_size}')\n",
    "            self.path_set_size = path_set_size\n",
    "            \n",
    "            # Populate the obs attribute\n",
    "            self.make_observations()\n",
    "            \n",
    "            # grab indexes of the held-out variants\n",
    "            for i,var in enumerate(np.unique(self.test_vars)):\n",
    "                res = self.score_var(var)\n",
    "                results[var][path_set_size] = res\n",
    "                print (f'Completed {i+1}/{self.test_vars.shape[0]} variants...')\n",
    "            print ()\n",
    "            \n",
    "            # Empty the obs attribute to reset it\n",
    "            self.obs = None     \n",
    "            \n",
    "        for var in results:\n",
    "            for size in results[var]:\n",
    "                results[var][size] = np.concatenate(results[var][size])\n",
    "                \n",
    "        # Plot\n",
    "        ncols = 3\n",
    "        nrows = int(np.ceil(len(results)/ncols))\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(9*ncols,6*nrows))\n",
    "        for i,var in enumerate(results):\n",
    "\n",
    "            var_kcat = np.log10(self.meta.kcat[np.where(np.array(self.meta.variant) == var)[0][0]])\n",
    "\n",
    "            path_set_sizes, y_pred = [],[]\n",
    "            for path_set_size in results[var]:\n",
    "\n",
    "                data = results[var][path_set_size]\n",
    "                y_pred += list(data)\n",
    "                path_set_sizes += [path_set_size]*data.shape[0]\n",
    "\n",
    "\n",
    "            df = pd.DataFrame({'path_set_size': path_set_sizes, 'Pred. log(kcat)': y_pred})\n",
    "\n",
    "            # Create violin plot with seaborn\n",
    "            axes.flatten()[i] = sns.violinplot(data=df, x='path_set_size', y='Pred. log(kcat)', ax=axes.flatten()[i])\n",
    "            axes.flatten()[i].axhline(var_kcat, ls='--', c='gray', label=r'TIS log($k_{cat}$)')\n",
    "            axes.flatten()[i].set_xlabel('')\n",
    "            axes.flatten()[i].set_ylabel('')\n",
    "            axes.flatten()[i].set_title(var, fontsize=22)\n",
    "            axes.flatten()[i].legend(fontsize=20)\n",
    "            axes.flatten()[i].tick_params(axis='both', labelsize=22)\n",
    "            fig.text(0.5, 0.07, 'Paths per prediction', ha='center', fontsize=32)\n",
    "            fig.text(0.05, 0.5, r'Predicted log($k_{cat}$)', va='center', rotation='vertical', fontsize=32)\n",
    "\n",
    "        # fig.tight_layout()\n",
    "        if figname:\n",
    "            fig.savefig(figname, dpi=300)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_train_var_pred(self, path_set_sizes=[10,100,1000], figname=False):\n",
    "        # Plots the distribution of predicted log(kcat) values \n",
    "        # for each variant included during training as a \n",
    "        # function of the path_set_size, which is the number of\n",
    "        # paths included in each observation of the variant\n",
    "        \n",
    "        results = {}\n",
    "        for var in self.train_vars:\n",
    "            d = {}\n",
    "            for size in path_set_sizes:\n",
    "                d[size] = []\n",
    "            results[var] = d\n",
    "            \n",
    "        for path_set_size in path_set_sizes:\n",
    "            \n",
    "            print (f'Testing path_set_size: {path_set_size}')\n",
    "            self.path_set_size = path_set_size\n",
    "            \n",
    "            # Populate the obs attribute\n",
    "            self.make_observations()\n",
    "            \n",
    "            # grab indexes of the held-out variants\n",
    "            for i,var in enumerate(np.unique(self.train_vars)):\n",
    "                res = self.score_var(var)\n",
    "                results[var][path_set_size] = res\n",
    "                print (f'Completed {i+1}/{self.train_vars.shape[0]} variants...')\n",
    "            print ()\n",
    "            \n",
    "            # Empty the obs attribute to reset it\n",
    "            self.obs = None     \n",
    "            \n",
    "        for var in results:\n",
    "            for size in results[var]:\n",
    "                results[var][size] = np.concatenate(results[var][size])\n",
    "                \n",
    "        # Plot\n",
    "        ncols = 3\n",
    "        nrows = int(np.ceil(len(results)/ncols))\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(9*ncols,6*nrows))\n",
    "        for i,var in enumerate(results):\n",
    "\n",
    "            var_kcat = np.log10(self.meta.kcat[np.where(np.array(self.meta.variant) == var)[0][0]])\n",
    "\n",
    "            path_set_sizes, y_pred = [],[]\n",
    "            for path_set_size in results[var]:\n",
    "\n",
    "                data = results[var][path_set_size]\n",
    "                y_pred += list(data)\n",
    "                path_set_sizes += [path_set_size]*data.shape[0]\n",
    "\n",
    "\n",
    "            df = pd.DataFrame({'path_set_size': path_set_sizes, 'Pred. log(kcat)': y_pred})\n",
    "\n",
    "            # Create violin plot with seaborn\n",
    "            axes.flatten()[i] = sns.violinplot(data=df, x='path_set_size', y='Pred. log(kcat)', ax=axes.flatten()[i])\n",
    "            axes.flatten()[i].axhline(var_kcat, ls='--', c='gray', label=r'TIS log($k_{cat}$)')\n",
    "            axes.flatten()[i].set_xlabel('')\n",
    "            axes.flatten()[i].set_ylabel('')\n",
    "            axes.flatten()[i].set_title(var, fontsize=22)\n",
    "            axes.flatten()[i].legend(fontsize=20)\n",
    "            axes.flatten()[i].tick_params(axis='both', labelsize=22)\n",
    "            fig.text(0.5, 0.07, 'Paths per prediction', ha='center', fontsize=32)\n",
    "            fig.text(0.05, 0.5, r'Predicted log($k_{cat}$)', va='center', rotation='vertical', fontsize=32)\n",
    "\n",
    "        # fig.tight_layout()\n",
    "        if figname:\n",
    "            fig.savefig(figname, dpi=300)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def spearman_test(self, verbose=False):\n",
    "        # Calculate Spearman rank correlation between predictions and \n",
    "        # TIS measured kcat values for variants in the test set\n",
    "        \n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html:\n",
    "        # Returns:\n",
    "        # res :\n",
    "        # SignificanceResult\n",
    "        # An object containing attributes:\n",
    "        #\n",
    "        #    statistic : \n",
    "        #    float or ndarray (2-D square)\n",
    "        #    Spearman correlation matrix or correlation coefficient (if only 2 \n",
    "        #    variables are given as parameters). Correlation matrix is square \n",
    "        #   with length equal to total number of variables (columns or rows) \n",
    "        #    in a and b combined.\n",
    "        #\n",
    "        #    pvalue :\n",
    "        #   float\n",
    "        #    The p-value for a hypothesis test whose null hypothesis is that \n",
    "        #    two samples have no ordinal correlation. See alternative above \n",
    "        #    for alternative hypotheses. pvalue has the same shape as statistic.\n",
    "        #\n",
    "        #    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "        \n",
    "\n",
    "        # Populate the obs attribute\n",
    "        self.make_observations()\n",
    "\n",
    "        variants, preds, targets = [], [], []\n",
    "        # grab indexes of the held-out variants\n",
    "        for i,var in enumerate(np.unique(self.test_vars)):\n",
    "            res = self.score_var(var)\n",
    "            res = np.concatenate(res)\n",
    "\n",
    "            variants.append(var)\n",
    "            preds.append(np.mean(res))\n",
    "            targets.append(np.log10(self.obs.kcat[np.where(self.obs.variant==var)[0][0]]))\n",
    "\n",
    "            print (f'Completed {i+1}/{np.unique(self.test_vars).shape[0]} variants...')\n",
    "\n",
    "        if verbose:\n",
    "            for i,var in enumerate(variants):\n",
    "                print (f'{var}: {preds[i]:.2f}   |   {targets[i]:.2f}')\n",
    "\n",
    "        res = spearmanr(preds, targets)#, alternative='greater')\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def spearman_train(self, verbose=False):\n",
    "        # Calculate Spearman rank correlation between predictions and \n",
    "        # TIS measured kcat values for variants in the train set\n",
    "        \n",
    "        # From https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html:\n",
    "        # Returns:\n",
    "        # res :\n",
    "        # SignificanceResult\n",
    "        # An object containing attributes:\n",
    "        #\n",
    "        #    statistic : \n",
    "        #    float or ndarray (2-D square)\n",
    "        #    Spearman correlation matrix or correlation coefficient (if only 2 \n",
    "        #    variables are given as parameters). Correlation matrix is square \n",
    "        #   with length equal to total number of variables (columns or rows) \n",
    "        #    in a and b combined.\n",
    "        #\n",
    "        #    pvalue :\n",
    "        #   float\n",
    "        #    The p-value for a hypothesis test whose null hypothesis is that \n",
    "        #    two samples have no ordinal correlation. See alternative above \n",
    "        #    for alternative hypotheses. pvalue has the same shape as statistic.\n",
    "        #\n",
    "        #    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "        \n",
    "\n",
    "        # Populate the obs attribute\n",
    "        self.make_observations()\n",
    "\n",
    "        variants, preds, targets = [], [], []\n",
    "        # grab indexes of the training variants\n",
    "        for i,var in enumerate(np.unique(self.train_vars)):\n",
    "            res = self.score_var(var)\n",
    "            res = np.concatenate(res)\n",
    "\n",
    "            variants.append(var)\n",
    "            preds.append(np.mean(res))\n",
    "            targets.append(np.log10(self.obs.kcat[np.where(self.obs.variant==var)[0][0]]))\n",
    "\n",
    "            print (f'Completed {i+1}/{np.unique(self.train_vars).shape[0]} variants...')\n",
    "\n",
    "        if verbose:\n",
    "            for i,var in enumerate(variants):\n",
    "                print (f'{var}: {preds[i]:.2f}   |   {targets[i]:.2f}')\n",
    "\n",
    "        res = spearmanr(preds, targets)#, alternative='greater')\n",
    "\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_dir(file):\n",
    "        # Returns direction under which file is stored\n",
    "        file_dir = '/'.join(file.split('/')[0:-1]) + '/'\n",
    "        return file_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def get_val_variants(model_file, output_file=None):\n",
    "        # Returns list of variants that were held out\n",
    "        # during training of the model saved to model_file\n",
    "        # INPUT:\n",
    "        # model_file -- full path to the file to which the \n",
    "        #               model was saved\n",
    "        # output_file -- the text file to which output was \n",
    "        #                written by the script that trained\n",
    "        #                and saved the model\n",
    "\n",
    "        if output_file == None:\n",
    "            output_file = ModelTest.get_dir(model_file) + 'transformer_1_output.txt'\n",
    "\n",
    "        if ModelTest.get_dir(model_file) != ModelTest.get_dir(output_file):\n",
    "            raise ValueError('model_file and output_file directories do not match.' +\\\n",
    "                             'Are you sure you have the right ones?')\n",
    "\n",
    "        # get the CV fold\n",
    "        fold = model_file.split('cvfold')[-1].split('.pt')[0]\n",
    "\n",
    "        # read the output file\n",
    "        with open(output_file, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # grab the variants used for training\n",
    "        train_vars = data.split(f\"epoch 1 | CV fold {fold}\")[0].split('Training variants:')[-1]\n",
    "\n",
    "        # grab the variants used for validation\n",
    "        val_vars = data.split(f\"Best loss achieved, saving model state to best_model_cvfold{fold}.pt\")[0]\n",
    "        val_vars = val_vars.split('Validation variants:')[-1]\n",
    "        \n",
    "        train_vars = train_vars.replace(\"' '\",',').replace(\"'\",'').replace('\\n ',',').split('[')[-1].split(']')[0].split(',')\n",
    "        val_vars = val_vars.replace(\"' '\",',').replace(\"'\",'').replace('\\n ',',').split('[')[-1].split(']')[0].split(',')\n",
    "\n",
    "        return np.array(train_vars), np.array(val_vars)\n",
    "        \n",
    "\n",
    "# model_file = '/data/karvelis03/dl_kcat/transformer_1s/job0/best_model_cvfold1.pt'\n",
    "# data_file = '/data/karvelis03/dl_kcat/data/total/tptrue_gsfalse_o-0dot4_0dot8_s1_2_3_4_5_r1_2_t-110_0_sub500_numNone.470000-111-70memnpy'\n",
    "\n",
    "# test = ModelTest(model_file, data_file)#, scaler=scaler)\n",
    "\n",
    "# start_time = time.time()\n",
    "# results = test.plot_test_var_pred()\n",
    "# print (f'\\n\\nRuntime: {time.time() - start_time}')\n",
    "# print (test.train_vars)\n",
    "# print (test.test_vars)\n",
    "# print (test.data.shape)\n",
    "# print (len(test.meta.variant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc642e6",
   "metadata": {},
   "source": [
    "# Spearman rank correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0b78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/data/karvelis03/dl_kcat/data/total/tptrue_gsfalse_o-0dot4_0dot8_s1_2_3_4_5_r1_2_t-35_75_sub500_numNone.550000-111-70memnpy'\n",
    "meta_file = '/data/karvelis03/dl_kcat/data/total/tptrue_gsfalse_o-0dot4_0dot8_s1_2_3_4_5_r1_2_t-35_75_sub500_numNone.550000-111-70metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d06e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "Completed 1/5 variants...\n",
      "Completed 2/5 variants...\n",
      "Completed 3/5 variants...\n",
      "Completed 4/5 variants...\n",
      "Completed 5/5 variants...\n",
      "Completed 1/5 variants...\n",
      "Completed 2/5 variants...\n",
      "Completed 3/5 variants...\n",
      "Completed 4/5 variants...\n",
      "Completed 5/5 variants...\n",
      "Completed 1/5 variants...\n",
      "Completed 2/5 variants...\n",
      "Completed 3/5 variants...\n",
      "Completed 4/5 variants...\n",
      "Completed 5/5 variants...\n",
      "Completed 1/5 variants...\n",
      "Completed 2/5 variants...\n",
      "Completed 3/5 variants...\n",
      "Completed 4/5 variants...\n",
      "Completed 5/5 variants...\n",
      "Completed 1/5 variants...\n",
      "Completed 2/5 variants...\n",
      "Completed 3/5 variants...\n",
      "Completed 4/5 variants...\n",
      "Completed 5/5 variants...\n",
      "[0.77142857 0.6        0.31428571 0.94285714 0.42857143 0.1\n",
      " 0.         0.7        0.1        0.8       ]\n",
      "0.4757142857142857\n"
     ]
    }
   ],
   "source": [
    "spearman_rs = []\n",
    "for model_file in glob('./best_model*'):\n",
    "    \n",
    "    test = ModelTest(model_file, data_file, meta_file=meta_file)\n",
    "    \n",
    "    test.path_set_size = 10\n",
    "    res = test.spearman_test()\n",
    "    spearman_rs.append(res.correlation)\n",
    "\n",
    "spearman_rs = np.array(spearman_rs)\n",
    "print (spearman_rs)\n",
    "print (np.mean(spearman_rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060d783",
   "metadata": {},
   "source": [
    "# CV Fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723195b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_file = '/data/karvelis03/dl_kcat/transformer_1s/denseweight/job12-1/stoch_labels/test/best_model_cvfold1.pt'\n",
    "test = ModelTest(model_file, data_file, meta_file=meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e47ea00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing path_set_size: 10\n",
      "Completed 1/49 variants...\n",
      "Completed 2/49 variants...\n",
      "Completed 3/49 variants...\n",
      "Completed 4/49 variants...\n",
      "Completed 5/49 variants...\n",
      "Completed 6/49 variants...\n",
      "Completed 7/49 variants...\n",
      "Completed 8/49 variants...\n",
      "Completed 9/49 variants...\n",
      "Completed 10/49 variants...\n",
      "Completed 11/49 variants...\n",
      "Completed 12/49 variants...\n",
      "Completed 13/49 variants...\n",
      "Completed 14/49 variants...\n",
      "Completed 15/49 variants...\n",
      "Completed 16/49 variants...\n",
      "Completed 17/49 variants...\n",
      "Completed 18/49 variants...\n",
      "Completed 19/49 variants...\n",
      "Completed 20/49 variants...\n",
      "Completed 21/49 variants...\n",
      "Completed 22/49 variants...\n",
      "Completed 23/49 variants...\n",
      "Completed 24/49 variants...\n",
      "Completed 25/49 variants...\n",
      "Completed 26/49 variants...\n",
      "Completed 27/49 variants...\n",
      "Completed 28/49 variants...\n",
      "Completed 29/49 variants...\n",
      "Completed 30/49 variants...\n",
      "Completed 31/49 variants...\n",
      "Completed 32/49 variants...\n",
      "Completed 33/49 variants...\n",
      "Completed 34/49 variants...\n",
      "Completed 35/49 variants...\n",
      "Completed 36/49 variants...\n",
      "Completed 37/49 variants...\n",
      "Completed 38/49 variants...\n",
      "Completed 39/49 variants...\n",
      "Completed 40/49 variants...\n",
      "Completed 41/49 variants...\n",
      "Completed 42/49 variants...\n",
      "Completed 43/49 variants...\n",
      "Completed 44/49 variants...\n",
      "Completed 45/49 variants...\n",
      "Completed 46/49 variants...\n",
      "Completed 47/49 variants...\n",
      "Completed 48/49 variants...\n",
      "Completed 49/49 variants...\n",
      "\n",
      "Testing path_set_size: 100\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 174.00 MiB (GPU 0; 11.92 GiB total capacity; 269.54 MiB already allocated; 14.06 MiB free; 272.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_train_var_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_cvfold1_train.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 171\u001b[0m, in \u001b[0;36mModelTest.plot_train_var_pred\u001b[0;34m(self, path_set_sizes, figname)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# grab indexes of the held-out variants\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_vars)):\n\u001b[0;32m--> 171\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     results[var][path_set_size] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_vars\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variants...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m, in \u001b[0;36mModelTest.score_var\u001b[0;34m(self, var)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m     70\u001b[0m     n_obs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     72\u001b[0m     total_obs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_obs\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# self.results[var][path_set_size].append(output)\u001b[39;00m\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/transformer_1s/denseweight/job12-1/stoch_labels/test/transformer_1.py:704\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, src_mask: Tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    688\u001b[0m \t\u001b[38;5;66;03m# INPUT\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \t\u001b[38;5;66;03m# src -- the input data of shape [n_paths, timesteps, features] where\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \t\u001b[38;5;66;03m# print ('Initial encoding')\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \t\u001b[38;5;66;03m# print (f'src.shape: {src.shape}')\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m \tsrc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size)\n\u001b[1;32m    705\u001b[0m \t\u001b[38;5;66;03m# print (f'src.shape: {src.shape}\\n\\n')\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \t\u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \t\u001b[38;5;66;03m# print ('Positional encoding')\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \t\u001b[38;5;66;03m# print (f'src.shape: {src.shape}')\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \tsrc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src)\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/karvelis03/dl_kcat/.env/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 174.00 MiB (GPU 0; 11.92 GiB total capacity; 269.54 MiB already allocated; 14.06 MiB free; 272.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "_ = test.plot_train_var_pred(figname='best_model_cvfold1_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e86d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = test.plot_test_var_pred(figname='best_model_cvfold1_train.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32571b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "SpearmanrResult(correlation=0.7714285714285715, pvalue=0.07239650145772594)\n"
     ]
    }
   ],
   "source": [
    "test.path_set_size = 10\n",
    "res = test.spearman_test()\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f225d88",
   "metadata": {},
   "source": [
    "# CV Fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfca337",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/data/karvelis03/dl_kcat/transformer_1s/denseweight/job12-1/stoch_labels/test/best_model_cvfold2.pt'\n",
    "test = ModelTest(model_file, data_file, meta_file=meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62f6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = test.plot_train_var_pred(figname='best_model_cvfold2_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754c3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = test.plot_test_var_pred(figname='best_model_cvfold2_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba4943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "SpearmanrResult(correlation=0.6, pvalue=0.20799999999999982)\n"
     ]
    }
   ],
   "source": [
    "test.path_set_size = 10\n",
    "res = test.spearman_test()\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddf574",
   "metadata": {},
   "source": [
    "# CV Fold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a471784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/data/karvelis03/dl_kcat/transformer_1s/denseweight/job12-1/stoch_labels/test/best_model_cvfold3.pt'\n",
    "test = ModelTest(model_file, data_file, meta_file=meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca3ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = test.plot_train_var_pred(figname='best_model_cvfold3_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test.plot_test_var_pred(figname='best_model_cvfold3_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc4cabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1/6 variants...\n",
      "Completed 2/6 variants...\n",
      "Completed 3/6 variants...\n",
      "Completed 4/6 variants...\n",
      "Completed 5/6 variants...\n",
      "Completed 6/6 variants...\n",
      "SpearmanrResult(correlation=0.3142857142857143, pvalue=0.5440932944606414)\n"
     ]
    }
   ],
   "source": [
    "test.path_set_size = 10\n",
    "res = test.spearman_test()\n",
    "print (res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_kcat",
   "language": "python",
   "name": "dl_kcat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
